{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annealing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from bisect import bisect_right,bisect_left\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class CyclicCosAnnealingLR(_LRScheduler):\n",
    "    r\"\"\"\n",
    "    Implements reset on milestones inspired from CosineAnnealingLR pytorch\n",
    "\n",
    "    Set the learning rate of each parameter group using a cosine annealing\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart in SGDR:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})(1 +\n",
    "        \\cos(\\frac{T_{cur}}{T_{max}}\\pi))\n",
    "\n",
    "    When last_epoch > last set milestone, lr is automatically set to \\eta_{min}\n",
    "\n",
    "    It has been proposed in\n",
    "    `SGDR: Stochastic Gradient Descent with Warm Restarts`_. Note that this only\n",
    "    implements the cosine annealing part of SGDR, and not the restarts.\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        milestones (list of ints): List of epoch indices. Must be increasing.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer,milestones, eta_min=0, last_epoch=-1):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError('Milestones should be a list of'\n",
    "                             ' increasing integers. Got {}', milestones)\n",
    "        self.eta_min = eta_min\n",
    "        self.milestones=milestones\n",
    "        super(CyclicCosAnnealingLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "\n",
    "        if self.last_epoch >= self.milestones[-1]:\n",
    "            return [self.eta_min for base_lr in self.base_lrs]\n",
    "\n",
    "        idx = bisect_right(self.milestones,self.last_epoch)\n",
    "\n",
    "        left_barrier = 0 if idx==0 else self.milestones[idx-1]\n",
    "        right_barrier = self.milestones[idx]\n",
    "\n",
    "        width = right_barrier - left_barrier\n",
    "        curr_pos = self.last_epoch- left_barrier\n",
    "\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "               (1 + math.cos(math.pi * curr_pos/ width)) / 2\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class CyclicLinearLR(_LRScheduler):\n",
    "    r\"\"\"\n",
    "    Implements reset on milestones inspired from Linear learning rate decay\n",
    "\n",
    "    Set the learning rate of each parameter group using a linear decay\n",
    "    schedule, where :math:`\\eta_{max}` is set to the initial lr and\n",
    "    :math:`T_{cur}` is the number of epochs since the last restart:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\eta_t = \\eta_{min} + (\\eta_{max} - \\eta_{min})(1 -\\frac{T_{cur}}{T_{max}})\n",
    "\n",
    "    When last_epoch > last set milestone, lr is automatically set to \\eta_{min}\n",
    "\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        milestones (list of ints): List of epoch indices. Must be increasing.\n",
    "        eta_min (float): Minimum learning rate. Default: 0.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "\n",
    "    .. _SGDR\\: Stochastic Gradient Descent with Warm Restarts:\n",
    "        https://arxiv.org/abs/1608.03983\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer,milestones, eta_min=0, last_epoch=-1):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError('Milestones should be a list of'\n",
    "                             ' increasing integers. Got {}', milestones)\n",
    "        self.eta_min = eta_min\n",
    "        self.milestones=milestones\n",
    "        super(CyclicLinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "\n",
    "        if self.last_epoch >= self.milestones[-1]:\n",
    "            return [self.eta_min for base_lr in self.base_lrs]\n",
    "\n",
    "        idx = bisect_right(self.milestones,self.last_epoch)\n",
    "\n",
    "        left_barrier = 0 if idx==0 else self.milestones[idx-1]\n",
    "        right_barrier = self.milestones[idx]\n",
    "\n",
    "        width = right_barrier - left_barrier\n",
    "        curr_pos = self.last_epoch- left_barrier\n",
    "\n",
    "        return [self.eta_min + (base_lr - self.eta_min) *\n",
    "               (1. - 1.0*curr_pos/ width)\n",
    "                for base_lr in self.base_lrs]\n",
    "\n",
    "'''\n",
    "#################################\n",
    "# TEST FOR SCHEDULER\n",
    "#################################\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2,2))\n",
    "milestones = [(2**x)*300 for x in range(30)]\n",
    "optimizer = optim.SGD(net.parameters(),lr=1e-3,momentum=0.9,weight_decay=0.0005,nesterov=True)\n",
    "scheduler = CyclicCosAnnealingLR(optimizer,milestones=milestones,eta_min=1e-6)\n",
    "\n",
    "lr_log = []\n",
    "\n",
    "for i in range(20*300):\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr_log.append(param_group['lr'])\n",
    "\n",
    "plt.plot(lr_log)\n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bharg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:262\u001b[0m\n\u001b[0;32m    258\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    260\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 262\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Bharg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:238\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    236\u001b[0m is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_load_library_flags:\n\u001b[1;32m--> 238\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mkernel32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibraryExW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0x00001100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m     last_error \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mget_last_error()\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m last_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m126\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SepConvNet(nn.Module):\n",
    "    def __init__(self, t1, f1, t2, f2, N1, N2, input_shape=[513, 345], NN=128):\n",
    "        super(SepConvNet, self).__init__()\n",
    "        self.vconv_left = nn.Conv2d(1, N1, kernel_size=(f1, t1), padding=0)\n",
    "        self.hconv_left = nn.Conv2d(N1, N2, kernel_size=(f2, t2))\n",
    "        self.hconv_right = nn.Conv2d(1, N1, kernel_size=(f2, t2))\n",
    "        self.vconv_right = nn.Conv2d(N1, N2, kernel_size=(f1, t1), padding=0)\n",
    "\n",
    "        self.fc0 = nn.Linear(N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2), NN)\n",
    "        self.fc1 = nn.Linear(NN, N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc2 = nn.Linear(NN, N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc3 = nn.Linear(NN, N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc4 = nn.Linear(NN, N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.hdeconv1 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2, t2))\n",
    "        self.hdeconv2 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2, t2))\n",
    "        self.hdeconv3 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2, t2))\n",
    "        self.hdeconv4 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2, t2))\n",
    "        self.vdeconv1 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1, t1))\n",
    "        self.vdeconv2 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1, t1))\n",
    "        self.vdeconv3 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1, t1))\n",
    "        self.vdeconv4 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1, t1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_left = self.vconv_left(x)\n",
    "        x_left = self.hconv_left(x_left)\n",
    "\n",
    "        x_right = self.hconv_right(x)\n",
    "        x_right = self.vconv_right(x_right)\n",
    "\n",
    "        x = x_left + x_right\n",
    "\n",
    "        s1 = x.shape\n",
    "\n",
    "        x = x.view(s1[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc0(x))\n",
    "\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x))\n",
    "        x3 = F.relu(self.fc3(x))\n",
    "        x4 = F.relu(self.fc4(x))\n",
    "\n",
    "        x1 = x1.view(s1[0], s1[1], s1[2], s1[3])\n",
    "        x2 = x2.view(s1[0], s1[1], s1[2], s1[3])\n",
    "        x3 = x3.view(s1[0], s1[1], s1[2], s1[3])\n",
    "        x4 = x4.view(s1[0], s1[1], s1[2], s1[3])\n",
    "\n",
    "        x1 = self.hdeconv1(x1)\n",
    "        x2 = self.hdeconv2(x2)\n",
    "        x3 = self.hdeconv3(x3)\n",
    "        x4 = self.hdeconv4(x4)\n",
    "\n",
    "        x1 = self.vdeconv1(x1)\n",
    "        x2 = self.vdeconv2(x2)\n",
    "        x3 = self.vdeconv3(x3)\n",
    "        x4 = self.vdeconv4(x4)\n",
    "\n",
    "        return x1, x2, x3, x4\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SepConvNet(nn.Module):\n",
    "    def __init__(self,t1,f1,t2,f2,N1,N2,input_shape=[513,862],NN=128):\n",
    "        super(SepConvNet, self).__init__()\n",
    "        self.vconv = nn.Conv2d(1,N1, kernel_size=(f1,t1),padding=0)\n",
    "        self.hconv = nn.Conv2d(N1,N2, kernel_size=(f2,t2))\n",
    "\n",
    "        self.fc0 = nn.Linear(N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2), NN)\n",
    "        self.fc1 = nn.Linear(NN,N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc2 = nn.Linear(NN,N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc3 = nn.Linear(NN,N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.fc4 = nn.Linear(NN,N2*(input_shape[0]-f1-f2+2)*(input_shape[1]-t1-t2+2))\n",
    "        self.hdeconv1 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2,t2))\n",
    "        self.hdeconv2 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2,t2))\n",
    "        self.hdeconv3 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2,t2))\n",
    "        self.hdeconv4 = nn.ConvTranspose2d(N2, N1, kernel_size=(f2,t2))\n",
    "        self.vdeconv1 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1,t1))\n",
    "        self.vdeconv2 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1,t1))\n",
    "        self.vdeconv3 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1,t1))\n",
    "        self.vdeconv4 = nn.ConvTranspose2d(N1, 1, kernel_size=(f1,t1))\n",
    "    def forward(self, x):\n",
    "        x = self.vconv(x)\n",
    "\n",
    "        x = self.hconv(x)\n",
    "\n",
    "        s1 = x.shape\n",
    "\n",
    "        x = x.view(s1[0],-1)\n",
    "\n",
    "\n",
    "\n",
    "        x = F.relu(self.fc0(x))\n",
    "\n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        x2 = F.relu(self.fc2(x))\n",
    "        x3 = F.relu(self.fc3(x))\n",
    "        x4 = F.relu(self.fc4(x))\n",
    "\n",
    "        x1 = x1.view(s1[0], s1[1],s1[2],s1[3])\n",
    "        x2 = x2.view(s1[0], s1[1],s1[2],s1[3])\n",
    "        x3 = x3.view(s1[0], s1[1],s1[2],s1[3])\n",
    "        x4 = x4.view(s1[0], s1[1],s1[2],s1[3])\n",
    "\n",
    "        x1 = self.hdeconv1(x1)\n",
    "        x2 = self.hdeconv2(x2)\n",
    "        x3 = self.hdeconv3(x3)\n",
    "        x4 = self.hdeconv4(x4)\n",
    "\n",
    "        x1 = self.vdeconv1(x1)\n",
    "        x2 = self.vdeconv2(x2)\n",
    "        x3 = self.vdeconv3(x3)\n",
    "        x4 = self.vdeconv4(x4)\n",
    "\n",
    "        return x1, x2, x3, x4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "#from skimage import io, transform\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class SourceSepTrain(Dataset):\n",
    "    def __init__(self, path='../Processed/Mixtures', transforms=None):\n",
    "    # assuming this to be the directory containing all the magnitude spectrum\n",
    "    #for all songs and all segments used in training\n",
    "        self.path = path\n",
    "        self.list = os.listdir(self.path)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mixture_path = '../Processed/Mixtures/'\n",
    "        bass_path = '../Processed/Bass/'\n",
    "        vocals_path = '../Processed/Vocals/'\n",
    "        drums_path = '../Processed/Drums/'\n",
    "        others_path = '../Processed/Others/'\n",
    "        mixture = torch.load(mixture_path+self.list[index])\n",
    "        #phase = torch.load(mixture_path+self.list[index]+'_p')\n",
    "        bass = torch.load(bass_path+self.list[index])\n",
    "        vocals = torch.load(vocals_path+self.list[index])\n",
    "        drums = torch.load(drums_path+self.list[index])\n",
    "        others = torch.load(others_path+self.list[index])\n",
    "        #print(mixture)\n",
    "        if self.transforms is not None:\n",
    "            mixture = self.transforms(mixture)\n",
    "\n",
    "            bass = self.transforms(bass)\n",
    "            vocals = self.transforms(vocals)\n",
    "            drums = self.transforms(drums)\n",
    "            others = self.transforms(others)\n",
    "        return (mixture,bass, vocals, drums, others)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list) # length of how much data you have\n",
    "\n",
    "\n",
    "class SourceSepVal(Dataset):\n",
    "    def __init__(self, path='../Val/Mixtures', transforms=None):\n",
    "        # assuming this to be the directory containing all the magnitude spectrum\n",
    "        #for all songs and all segments used in training\n",
    "        self.path = path\n",
    "        self.list = os.listdir(self.path)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # stuff\n",
    "        mixture_path = '../Val/Mixtures/'\n",
    "        bass_path = '../Val/Bass/'\n",
    "        vocals_path = '../Val/Vocals/'\n",
    "        drums_path = '../Val/Drums/'\n",
    "        others_path = '../Val/Others/'\n",
    "\n",
    "        mixture = torch.load(mixture_path+self.list[index])\n",
    "        #phase = torch.load(mixture_path+self.list[index]+'_p')\n",
    "        bass = torch.load(bass_path+self.list[index])\n",
    "        vocals = torch.load(vocals_path+self.list[index])\n",
    "        drums = torch.load(drums_path+self.list[index])\n",
    "        others = torch.load(others_path+self.list[index])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            mixture = self.transforms(mixture)\n",
    "            bass = self.transforms(bass)\n",
    "            vocals = self.transforms(vocals)\n",
    "            drums = self.transforms(drums)\n",
    "            others = self.transforms(others)\n",
    "\n",
    "        return (mixture,bass, vocals, drums, others)\n",
    "    def __len__(self):\n",
    "        return len(self.list)\n",
    "\n",
    "class SourceSepTest(Dataset):\n",
    "    def __init__(self, path='../Val/Mixtures',transforms=None):\n",
    "        # assuming this to be the directory containing all the magnitude spectrum\n",
    "        #for all songs and all segments used in training\n",
    "        self.path = path\n",
    "        self.list = os.listdir(self.path)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        mixture_path = '../Val/Mixtures/'\n",
    "        bass_path = '../Val/Bass/'\n",
    "        vocals_path = '../Val/Vocals/'\n",
    "        drums_path = '../Val/Drums/'\n",
    "        others_path = '../Val/Others/'\n",
    "        phase_path = '../Val/Phases/'\n",
    "\n",
    "        phase_file=self.list[index].replace('_m','_p')\n",
    "        phase_file=phase_file.replace('.pt','.npy')\n",
    "        mixture = torch.load(mixture_path+self.list[index])\n",
    "        #phase = np.load(phase_path+phase_file)\n",
    "        bass = torch.load(bass_path+self.list[index])\n",
    "        vocals = torch.load(vocals_path+self.list[index])\n",
    "        drums = torch.load(drums_path+self.list[index])\n",
    "        others = torch.load(others_path+self.list[index])\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            mixture = self.transforms(mixture)\n",
    "            bass = self.transforms(bass)\n",
    "            vocals = self.transforms(vocals)\n",
    "            drums = self.transforms(drums)\n",
    "            others = self.transforms(others)\n",
    "\n",
    "        return (mixture,phase_file,self.list[index])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Paths\n",
    "base_path = \"../dsd100/\"\n",
    "path_mixtures = os.path.join(base_path, \"Mixtures/Dev/\")\n",
    "path_sources = os.path.join(base_path, \"Sources/Dev/\")\n",
    "processed_path = \"../Processed/\"\n",
    "\n",
    "destination_path = os.path.join(processed_path, \"Mixtures\")\n",
    "phase_path = os.path.join(processed_path, \"Phases\")\n",
    "bass_path = os.path.join(processed_path, \"Bass\")\n",
    "vocals_path = os.path.join(processed_path, \"Vocals\")\n",
    "drums_path = os.path.join(processed_path, \"Drums\")\n",
    "others_path = os.path.join(processed_path, \"Others\")\n",
    "source_dest_paths = [vocals_path, bass_path, drums_path, others_path]\n",
    "\n",
    "# Validation Paths\n",
    "path_val_mixtures = os.path.join(base_path, \"Mixtures/Test/\")\n",
    "path_val_sources = os.path.join(base_path, \"Sources/Test/\")\n",
    "val_path = \"../Val/\"\n",
    "\n",
    "validation_path = os.path.join(val_path, \"Mixtures\")\n",
    "val_phase_path = os.path.join(val_path, \"Phases\")\n",
    "val_bass_path = os.path.join(val_path, \"Bass\")\n",
    "val_vocals_path = os.path.join(val_path, \"Vocals\")\n",
    "val_drums_path = os.path.join(val_path, \"Drums\")\n",
    "val_others_path = os.path.join(val_path, \"Others\")\n",
    "source_val_paths = [val_vocals_path, val_bass_path, val_drums_path, val_others_path]\n",
    "\n",
    "# Test Paths (same structure as validation)\n",
    "path_test_mixtures = path_val_mixtures\n",
    "path_test_sources = path_val_sources\n",
    "test_path = \"../Test/\"\n",
    "\n",
    "testing_path = os.path.join(test_path, \"Mixtures\")\n",
    "test_phase_path = os.path.join(test_path, \"Phases\")\n",
    "test_bass_path = os.path.join(test_path, \"Bass\")\n",
    "test_vocals_path = os.path.join(test_path, \"Vocals\")\n",
    "test_drums_path = os.path.join(test_path, \"Drums\")\n",
    "test_others_path = os.path.join(test_path, \"Others\")\n",
    "source_test_paths = [test_vocals_path, test_bass_path, test_drums_path, test_others_path]\n",
    "\n",
    "\n",
    "def process(file_path, direc, destination_path, phase_bool, destination_phase_path):\n",
    "    \"\"\"\n",
    "    Process audio files: segment, compute STFT, and save magnitude and phase data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get duration and calculate segments\n",
    "        duration = librosa.get_duration(filename=file_path)\n",
    "        max_segments = int(duration // 0.3)\n",
    "        regex = re.compile(r'\\d+')\n",
    "        index = regex.findall(direc)\n",
    "        if not index:\n",
    "            raise ValueError(\"Directory name does not contain valid index.\")\n",
    "        \n",
    "        for start in range(max_segments):\n",
    "            wave_array, fs = librosa.load(file_path, sr=44100, offset=start * 0.3, duration=0.3)\n",
    "            mag, phase = librosa.magphase(librosa.stft(wave_array, n_fft=1024, hop_length=256, window='hann', center=True))\n",
    "            \n",
    "            os.makedirs(destination_path, exist_ok=True)\n",
    "            torch.save(torch.from_numpy(np.expand_dims(mag, axis=0)), os.path.join(destination_path, f\"{index[0]}_{start}_m.pt\"))\n",
    "            \n",
    "            if phase_bool:\n",
    "                os.makedirs(destination_phase_path, exist_ok=True)\n",
    "                np.save(os.path.join(destination_phase_path, f\"{index[0]}_{start}_p.npy\"), phase)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "\n",
    "def process_directory(base_path, destination_paths, phase_bool=False, phase_path=None):\n",
    "    \"\"\"\n",
    "    Process all files in a directory structure.\n",
    "    \"\"\"\n",
    "    for subdirs, dirs, files in os.walk(base_path):\n",
    "        for direc in dirs:\n",
    "            print(f\"Processing directory: {direc}\")\n",
    "            for s, d, f in os.walk(os.path.join(base_path, direc)):\n",
    "                if not f:\n",
    "                    continue\n",
    "                for i, file in enumerate(f[:len(destination_paths)]):  # Ensure alignment with paths\n",
    "                    file_path = os.path.join(base_path, direc, file)\n",
    "                    process(file_path, direc, destination_paths[i], phase_bool, phase_path)\n",
    "\n",
    "\n",
    "# ------------------------- Training Data -------------------------\n",
    "print(\"Processing training data...\")\n",
    "process_directory(path_mixtures, [destination_path], phase_bool=True, phase_path=phase_path)\n",
    "process_directory(path_sources, source_dest_paths, phase_bool=False)\n",
    "\n",
    "# ------------------------ Validation Data ------------------------\n",
    "print(\"Processing validation data...\")\n",
    "process_directory(path_val_mixtures, [validation_path], phase_bool=True, phase_path=val_phase_path)\n",
    "process_directory(path_val_sources, source_val_paths, phase_bool=False)\n",
    "\n",
    "# -------------------------- Test Data ---------------------------\n",
    "print(\"Processing test data...\")\n",
    "process_directory(path_test_mixtures, [testing_path], phase_bool=True, phase_path=test_phase_path)\n",
    "process_directory(path_test_sources, source_test_paths, phase_bool=False)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from build_model_original import SepConvNet  # Ensure this file and model are correctly implemented\n",
    "from data_loader import SourceSepTrain, SourceSepVal  # Ensure these are correctly implemented\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Paths and Parameters\n",
    "mean_var_path = \"../Processed/\"\n",
    "if not os.path.exists('Weights'):\n",
    "    os.makedirs('Weights')\n",
    "\n",
    "inp_size = [513, 52]\n",
    "t1, f1 = 1, 513\n",
    "t2, f2 = 15, 1\n",
    "N1, N2, NN = 50, 30, 128\n",
    "alpha, beta, beta_vocals = 0.005, 0.05, 0.08\n",
    "batch_size, num_epochs = 30, 50\n",
    "\n",
    "writer = SummaryWriter()  # TensorBoard writer\n",
    "\n",
    "\n",
    "# Utility Classes\n",
    "class Average:\n",
    "    \"\"\"Tracks and computes the average of values over time.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val\n",
    "        self.count += n\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        return self.sum / self.count\n",
    "\n",
    "\n",
    "# Custom Loss Function\n",
    "class MixedSquaredError(nn.Module):\n",
    "    \"\"\"Custom loss function for the source separation task.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(MixedSquaredError, self).__init__()\n",
    "\n",
    "    def forward(self, pred_bass, pred_vocals, pred_drums, pred_others, gt_bass, gt_vocals, gt_drums, gt_others):\n",
    "        L_sq = torch.sum((pred_bass - gt_bass).pow(2)) + torch.sum((pred_vocals - gt_vocals).pow(2)) + torch.sum((pred_drums - gt_drums).pow(2))\n",
    "        L_other = torch.sum((pred_bass - gt_others).pow(2)) + torch.sum((pred_drums - gt_others).pow(2))\n",
    "        L_othervocals = torch.sum((pred_vocals - gt_others).pow(2))\n",
    "        L_diff = torch.sum((pred_bass - pred_vocals).pow(2)) + torch.sum((pred_bass - pred_drums).pow(2)) + torch.sum((pred_vocals - pred_drums).pow(2))\n",
    "        return L_sq - alpha * L_diff - beta * L_other - beta_vocals * L_othervocals\n",
    "\n",
    "\n",
    "# Time-Frequency Masking Function\n",
    "def TimeFreqMasking(bass, vocals, drums, others, cuda=False):\n",
    "    \"\"\"Applies time-frequency masking.\"\"\"\n",
    "    den = torch.abs(bass) + torch.abs(vocals) + torch.abs(drums) + torch.abs(others) + 1e-8\n",
    "    bass = torch.abs(bass) / den\n",
    "    vocals = torch.abs(vocals) / den\n",
    "    drums = torch.abs(drums) / den\n",
    "    others = torch.abs(others) / den\n",
    "    return bass, vocals, drums, others\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train():\n",
    "    cuda = torch.cuda.is_available()\n",
    "    net = SepConvNet(t1, f1, t2, f2, N1, N2, inp_size, NN)\n",
    "    criterion = MixedSquaredError()\n",
    "    if cuda:\n",
    "        net = net.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[60, 120])\n",
    "\n",
    "    print(\"Preparing training data ...\")\n",
    "    train_loader = DataLoader(SourceSepTrain(transforms=None), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(SourceSepVal(transforms=None), batch_size=batch_size, shuffle=False)\n",
    "    print(\"Data preparation done.\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        scheduler.step()\n",
    "        train_loss = Average()\n",
    "\n",
    "        # Training Phase\n",
    "        net.train()\n",
    "        for inp, gt_bass, gt_vocals, gt_drums, gt_others in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "            mean, std = torch.mean(inp), torch.std(inp)\n",
    "            inp_n = (inp - mean) / std\n",
    "\n",
    "            inp, inp_n = Variable(inp), Variable(inp_n)\n",
    "            gt_bass, gt_vocals, gt_drums, gt_others = map(Variable, (gt_bass, gt_vocals, gt_drums, gt_others))\n",
    "            if cuda:\n",
    "                inp, inp_n, gt_bass, gt_vocals, gt_drums, gt_others = inp.cuda(), inp_n.cuda(), gt_bass.cuda(), gt_vocals.cuda(), gt_drums.cuda(), gt_others.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            o_bass, o_vocals, o_drums, o_others = net(inp_n)\n",
    "            mask_bass, mask_vocals, mask_drums, mask_others = TimeFreqMasking(o_bass, o_vocals, o_drums, o_others, cuda)\n",
    "\n",
    "            pred_bass, pred_vocals, pred_drums, pred_others = inp * mask_bass, inp * mask_vocals, inp * mask_drums, inp * mask_others\n",
    "            loss = criterion(pred_bass, pred_vocals, pred_drums, pred_others, gt_bass, gt_vocals, gt_drums, gt_others)\n",
    "            writer.add_scalar('Train Loss', loss.item(), epoch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.update(loss.item(), inp.size(0))\n",
    "\n",
    "        # Validation Phase\n",
    "        val_loss = Average()\n",
    "        net.eval()\n",
    "        for val_inp, gt_bass, gt_vocals, gt_drums, gt_others in val_loader:\n",
    "            with torch.no_grad():\n",
    "                val_mean, val_std = torch.mean(val_inp), torch.std(val_inp)\n",
    "                val_inp_n = (val_inp - val_mean) / val_std\n",
    "\n",
    "                if cuda:\n",
    "                    val_inp_n, gt_bass, gt_vocals, gt_drums, gt_others = val_inp_n.cuda(), gt_bass.cuda(), gt_vocals.cuda(), gt_drums.cuda(), gt_others.cuda()\n",
    "\n",
    "                o_bass, o_vocals, o_drums, o_others = net(val_inp_n)\n",
    "                mask_bass, mask_vocals, mask_drums, mask_others = TimeFreqMasking(o_bass, o_vocals, o_drums, o_others, cuda)\n",
    "\n",
    "                pred_bass, pred_vocals, pred_drums, pred_others = val_inp * mask_bass, val_inp * mask_vocals, val_inp * mask_drums, val_inp * mask_others\n",
    "                vloss = criterion(pred_bass, pred_vocals, pred_drums, pred_others, gt_bass, gt_vocals, gt_drums, gt_others)\n",
    "                writer.add_scalar('Validation Loss', vloss.item(), epoch)\n",
    "                val_loss.update(vloss.item(), val_inp.size(0))\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss.avg:.4f}, Validation Loss: {val_loss.avg:.4f}\")\n",
    "        torch.save(net.state_dict(), f\"Weights/Weights_epoch{epoch+1}_valLoss{val_loss.avg:.4f}.pth\")\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# Test Function\n",
    "def test(model):\n",
    "    \"\"\"Function to test the trained model.\"\"\"\n",
    "    model.eval()\n",
    "    # Implement test logic here\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bharg\\AppData\\Local\\Temp\\ipykernel_4060\\2538430274.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Val/Mixtures'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load the test set\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m test_set \u001b[38;5;241m=\u001b[39m \u001b[43mSourceSepTest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Replace with any transformations, if applicable\u001b[39;00m\n\u001b[0;32m     47\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Test loop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bharg\\Downloads\\audio-source-separation-master\\code\\data_loader.py:82\u001b[0m, in \u001b[0;36mSourceSepTest.__init__\u001b[1;34m(self, path, transforms)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal/Mixtures\u001b[39m\u001b[38;5;124m'\u001b[39m,transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# assuming this to be the directory containing all the magnitude spectrum\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m#for all songs and all segments used in training\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m path\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m transforms\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Val/Mixtures'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "from build_model_original import SepConvNet  # Ensure this file is implemented correctly\n",
    "from torch.utils.data import DataLoader\n",
    "from data_loader import SourceSepTest  # Ensure this dataset class is implemented correctly\n",
    "from post_processing import reconstruct  # Ensure this function reconstructs the audio correctly\n",
    "from train_model import TimeFreqMasking  # Reuse the masking function from training\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Main testing script\n",
    "if __name__ == '__main__':\n",
    "    # Model and dataset configurations\n",
    "    inp_size = [513, 52]\n",
    "    t1, f1 = 1, 513\n",
    "    t2, f2 = 15, 1\n",
    "    N1, N2, NN = 50, 30, 128\n",
    "    batch_size = 1\n",
    "\n",
    "    # Directory configurations\n",
    "    destination_path = '../AudioResults/'\n",
    "    phase_path = '../Val/Phases/'\n",
    "    vocals_directory = os.path.join(destination_path, 'vocals')\n",
    "    drums_directory = os.path.join(destination_path, 'drums')\n",
    "    bass_directory = os.path.join(destination_path, 'bass')\n",
    "    others_directory = os.path.join(destination_path, 'others')\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    for directory in [destination_path, vocals_directory, drums_directory, bass_directory, others_directory]:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    # Load the model\n",
    "    net = SepConvNet(t1, f1, t2, f2, N1, N2, inp_size, NN)\n",
    "    \n",
    "    weights_path = '../Weights/Weights_epoch10_valLoss27994.5971.pth'  # Replace with your best model's weight file path\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(f\"Model weights not found at {weights_path}\")\n",
    "    \n",
    "    # Load the trained weights\n",
    "    net.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "    net.eval()\n",
    "\n",
    "    # Load the test set\n",
    "    test_set = SourceSepTest(transforms=None)  # Replace with any transformations, if applicable\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Test loop\n",
    "    for i, (test_inp, test_phase_file, file_str) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Testing\"):\n",
    "        print(f'Testing sample {i + 1}/{len(test_loader)}')\n",
    "\n",
    "        # Load the phase file for reconstruction\n",
    "        test_phase_path = os.path.join(phase_path, test_phase_file[0])\n",
    "        if not os.path.exists(test_phase_path):\n",
    "            raise FileNotFoundError(f\"Phase file not found at {test_phase_path}\")\n",
    "        test_phase = np.load(test_phase_path)  # NumPy array\n",
    "\n",
    "        # Normalize input\n",
    "        mean, std = torch.mean(test_inp), torch.std(test_inp)\n",
    "        test_inp_n = (test_inp - mean) / std\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            bass_mag, vocals_mag, drums_mag, others_mag = net(test_inp_n)\n",
    "            bass_mag, vocals_mag, drums_mag, others_mag = TimeFreqMasking(bass_mag, vocals_mag, drums_mag, others_mag)\n",
    "\n",
    "            # Apply masks to the input spectrogram\n",
    "            bass_mag = bass_mag * test_inp\n",
    "            vocals_mag = vocals_mag * test_inp\n",
    "            drums_mag = drums_mag * test_inp\n",
    "            others_mag = others_mag * test_inp\n",
    "\n",
    "        # Extract indices from the file string\n",
    "        regex = re.compile(r'\\d+')\n",
    "        indices = regex.findall(file_str[0])\n",
    "        if len(indices) < 2:\n",
    "            raise ValueError(f\"Expected at least two indices in the file string, found: {file_str[0]}\")\n",
    "        index_start, index_end = indices[0], indices[1]\n",
    "\n",
    "        # Reconstruct the audio files\n",
    "        reconstruct(\n",
    "            test_phase,                          # NumPy array\n",
    "            bass_mag.squeeze().cpu().numpy(),   # PyTorch tensor to NumPy array\n",
    "            vocals_mag.squeeze().cpu().numpy(),\n",
    "            drums_mag.squeeze().cpu().numpy(),\n",
    "            others_mag.squeeze().cpu().numpy(),\n",
    "            index_start,                        # Start index from the file name\n",
    "            index_end,                          # End index from the file name\n",
    "            destination_path                    # Path to save the results\n",
    "        )\n",
    "\n",
    "    print(\"Testing complete. Check the results in the specified output directories.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import soundfile as sf  # Import soundfile for writing audio files\n",
    "\n",
    "def reconstruct(phase, bass_mag, vocals_mag, drums_mag, others_mag, song_num, segment_num, destination_path):\n",
    "    # Ensure bass_mag, vocals_mag, drums_mag, others_mag are NumPy arrays\n",
    "    if isinstance(bass_mag, torch.Tensor):\n",
    "        bass_mag = bass_mag.detach().cpu().numpy()  # Convert to NumPy after detaching\n",
    "    if isinstance(vocals_mag, torch.Tensor):\n",
    "        vocals_mag = vocals_mag.detach().cpu().numpy()\n",
    "    if isinstance(drums_mag, torch.Tensor):\n",
    "        drums_mag = drums_mag.detach().cpu().numpy()\n",
    "    if isinstance(others_mag, torch.Tensor):\n",
    "        others_mag = others_mag.detach().cpu().numpy()\n",
    "\n",
    "    # Ensure phase is a NumPy array if not already\n",
    "    phase = np.asarray(phase)\n",
    "\n",
    "    # Retrieve complex STFT\n",
    "    vocals = vocals_mag * phase\n",
    "    bass = bass_mag * phase\n",
    "    drums = drums_mag * phase\n",
    "    others = others_mag * phase\n",
    "\n",
    "    # Perform ISTFT\n",
    "    vocals_audio = librosa.istft(vocals, win_length=1024, hop_length=256, window='hann', center=True)\n",
    "    bass_audio = librosa.istft(bass, win_length=1024, hop_length=256, window='hann', center=True)\n",
    "    drums_audio = librosa.istft(drums, win_length=1024, hop_length=256, window='hann', center=True)\n",
    "    others_audio = librosa.istft(others, win_length=1024, hop_length=256, window='hann', center=True)\n",
    "\n",
    "    # Save as wav files using soundfile.write instead of librosa.output.write_wav\n",
    "    sf.write(os.path.join(destination_path, 'vocals', f'{song_num}_{segment_num}.wav'), vocals_audio, 44100)\n",
    "    sf.write(os.path.join(destination_path, 'bass', f'{song_num}_{segment_num}.wav'), bass_audio, 44100)\n",
    "    sf.write(os.path.join(destination_path, 'drums', f'{song_num}_{segment_num}.wav'), drums_audio, 44100)\n",
    "    sf.write(os.path.join(destination_path, 'others', f'{song_num}_{segment_num}.wav'), others_audio, 44100)\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mir_eval  \n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "\n",
    "\n",
    "####################### MODIFY ##############################\n",
    "#### additional for loop to evaluate multiple songs #########\n",
    "# increase step to decrease time\n",
    "step  = 10\n",
    "bass_gt_path = 'bass.wav'\n",
    "bass_rec_path = 'bass_rec.wav'\n",
    "vocal_gt_path = 'vocals.wav'\n",
    "vocal_rec_path = 'vocals_rec.wav'\n",
    "drums_gt_path = 'drums.wav'\n",
    "drums_rec_path = 'drums_rec.wav'\n",
    "other_gt_path = 'other.wav'\n",
    "other_rec_path = 'other_rec.wav'\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "bass_gt, rate11 = librosa.load(bass_gt_path,sr=44100, offset=30*0.3,duration = 170*0.3)\n",
    "bass_rec, rate21 = librosa.load(bass_rec_path,sr=44100)\n",
    "\n",
    "vocals_gt, rate12 = librosa.load(vocal_gt_path,sr=44100, offset=30*0.3,duration = 170*0.3)\n",
    "vocals_rec, rate22 = librosa.load(vocal_rec_path,sr=44100)\n",
    "\n",
    "drums_gt, rate13 = librosa.load(drums_gt_path,sr=44100, offset=30*0.3,duration = 170*0.3)\n",
    "drums_rec, rate23 = librosa.load(drums_rec_path,sr=44100)\n",
    "\n",
    "other_gt, rate14 = librosa.load(other_gt_path,sr=44100, offset=30*0.3,duration = 170*0.3)\n",
    "other_rec, rate24 = librosa.load(other_rec_path,sr=44100)\n",
    "\n",
    "\n",
    "bass_gt = bass_gt[0:bass_rec.shape[0]:step]\n",
    "bass_gt = np.transpose(bass_gt.reshape(len(bass_gt), 1))\n",
    "\n",
    "vocals_gt = vocals_gt[0:vocals_rec.shape[0]:step]\n",
    "vocals_gt = np.transpose(vocals_gt.reshape(len(vocals_gt), 1))\n",
    "\n",
    "drums_gt = drums_gt[0:drums_rec.shape[0]:step]\n",
    "drums_gt = np.transpose(drums_gt.reshape(len(drums_gt), 1))\n",
    "\n",
    "other_gt = other_gt[0:other_rec.shape[0]:step]\n",
    "other_gt = np.transpose(other_gt.reshape(len(other_gt), 1))\n",
    "\n",
    "final_gt = np.concatenate((bass_gt, vocals_gt, drums_gt, other_gt), axis = 0)\n",
    "print(final_gt.shape)\n",
    "\n",
    "\n",
    "bass_rec = bass_rec[0:bass_rec.shape[0]:step]\n",
    "bass_rec = np.transpose(bass_rec.reshape(len(bass_rec), 1))\n",
    "\n",
    "vocals_rec = vocals_rec[0:vocals_rec.shape[0]:step]\n",
    "vocals_rec = np.transpose(vocals_rec.reshape(len(vocals_rec), 1))\n",
    "\n",
    "drums_rec = drums_rec[0:drums_rec.shape[0]:step]\n",
    "drums_rec = np.transpose(drums_rec.reshape(len(drums_rec), 1))\n",
    "\n",
    "other_rec = other_rec[0:other_rec.shape[0]:step]\n",
    "other_rec = np.transpose(other_rec.reshape(len(other_rec), 1))\n",
    "\n",
    "final_rec = np.concatenate((bass_rec, vocals_rec, drums_rec, other_rec), axis = 0)\n",
    "print(final_rec.shape)\n",
    "\n",
    "\n",
    "\n",
    "SDR, SIR, SAR, perm = mir_eval.separation.bss_eval_sources(final_gt, final_rec)\n",
    "\n",
    "print(SDR)\n",
    "print(SIR)\n",
    "print(SAR)\n",
    "print(perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf  # Use soundfile for writing wav files\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "destination_path = '../Recovered_Songs_bigger5/'\n",
    "vocals_directory = '../AudioResults/vocals'\n",
    "drums_directory = '../AudioResults/drums'\n",
    "bass_directory = '../AudioResults/bass'\n",
    "others_directory = '../AudioResults/others'\n",
    "test_songs_list = []\n",
    "vocals_list = []\n",
    "\n",
    "# Create necessary directories\n",
    "if not os.path.exists(destination_path):\n",
    "    os.makedirs(destination_path)\n",
    "if not os.path.exists(vocals_directory):\n",
    "    os.makedirs(vocals_directory)\n",
    "if not os.path.exists(drums_directory):\n",
    "    os.makedirs(drums_directory)\n",
    "if not os.path.exists(bass_directory):\n",
    "    os.makedirs(bass_directory)\n",
    "if not os.path.exists(others_directory):\n",
    "    os.makedirs(others_directory)\n",
    "\n",
    "# Collect all unique test songs\n",
    "for subdirs, dirs, files in os.walk(vocals_directory):\n",
    "    print('Finding list of songs')\n",
    "    for file in files:\n",
    "        regex = re.compile(r'\\d+')\n",
    "        index = regex.findall(file)\n",
    "        if index and index[0] not in test_songs_list:\n",
    "            test_songs_list.append(index[0])\n",
    "\n",
    "# Iterate through each test song and combine audio segments\n",
    "for test_songs in test_songs_list:\n",
    "    combined_vocals = np.array([])\n",
    "    sr = None\n",
    "    print(f'Testing: {test_songs}')\n",
    "    print('Stitching Vocals')\n",
    "\n",
    "    # Get list of vocals files for the current song\n",
    "    vocals_list = sorted(glob.glob(os.path.join(vocals_directory, test_songs + \"*\")))\n",
    "    vocals_path = os.path.join(destination_path, 'vocals')\n",
    "    if not os.path.exists(vocals_path):\n",
    "        os.makedirs(vocals_path)\n",
    "\n",
    "    sound_output_path = os.path.join(vocals_path, f'{test_songs}.wav')\n",
    "\n",
    "    # Combine vocals segments\n",
    "    for segment in vocals_list:\n",
    "        seg, sr = librosa.load(segment, sr=44100)\n",
    "        assert sr == 44100\n",
    "        combined_vocals = np.append(combined_vocals, seg)\n",
    "    \n",
    "    # Save combined vocals using soundfile.write\n",
    "    sf.write(sound_output_path, combined_vocals, sr)\n",
    "\n",
    "    print('Stitching Bass')\n",
    "    combined_bass = np.array([])\n",
    "    bass_list = sorted(glob.glob(os.path.join(bass_directory, test_songs + \"*\")))\n",
    "    bass_path = os.path.join(destination_path, 'bass')\n",
    "    if not os.path.exists(bass_path):\n",
    "        os.makedirs(bass_path)\n",
    "    \n",
    "    sound_output_path = os.path.join(bass_path, f'{test_songs}.wav')\n",
    "\n",
    "    # Combine bass segments\n",
    "    for segment in bass_list:\n",
    "        seg, sr = librosa.load(segment, sr=44100)\n",
    "        assert sr == 44100\n",
    "        combined_bass = np.append(combined_bass, seg)\n",
    "    \n",
    "    # Save combined bass using soundfile.write\n",
    "    sf.write(sound_output_path, combined_bass, sr)\n",
    "\n",
    "    print('Stitching Drums')\n",
    "    combined_drums = np.array([])\n",
    "    drums_list = sorted(glob.glob(os.path.join(drums_directory, test_songs + \"*\")))\n",
    "    drums_path = os.path.join(destination_path, 'drums')\n",
    "    if not os.path.exists(drums_path):\n",
    "        os.makedirs(drums_path)\n",
    "    \n",
    "    sound_output_path = os.path.join(drums_path, f'{test_songs}.wav')\n",
    "\n",
    "    # Combine drums segments\n",
    "    for segment in drums_list:\n",
    "        seg, sr = librosa.load(segment, sr=44100)\n",
    "        combined_drums = np.append(combined_drums, seg)\n",
    "    \n",
    "    # Save combined drums using soundfile.write\n",
    "    sf.write(sound_output_path, combined_drums, sr)\n",
    "\n",
    "    print('Stitching Others')\n",
    "    combined_others = np.array([])\n",
    "    others_list = sorted(glob.glob(os.path.join(others_directory, test_songs + \"*\")))\n",
    "    others_path = os.path.join(destination_path, 'others')\n",
    "    if not os.path.exists(others_path):\n",
    "        os.makedirs(others_path)\n",
    "    \n",
    "    sound_output_path = os.path.join(others_path, f'{test_songs}.wav')\n",
    "\n",
    "    # Combine other segments\n",
    "    for segment in others_list:\n",
    "        seg, sr = librosa.load(segment, sr=44100)\n",
    "        combined_others = np.append(combined_others, seg)\n",
    "    \n",
    "    # Save combined others using soundfile.write\n",
    "    sf.write(sound_output_path, combined_others, sr)\n",
    "\n",
    "print(\"All songs have been stitched and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
